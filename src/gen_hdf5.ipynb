{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "#dataset_root = '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/'\n",
    "#files = sorted( filter( os.path.isdir,\n",
    "#                        glob.glob(dataset_root + '*') ) )\n",
    "#for file in files:\n",
    "#    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "def plotMesh(meshfile, arr, property):\n",
    "    p = pv.Plotter()\n",
    "    p.enable()\n",
    "    p.enable_anti_aliasing()\n",
    "\n",
    "    mesh = pv.read(meshfile)\n",
    "    mesh[property] = arr[:]\n",
    "    mesh.set_active_scalars(property)\n",
    "    p.add_mesh(mesh, opacity=0.85, render=True, cmap='plasma')\n",
    "    p.add_mesh(mesh.contour(), color=\"white\", line_width=2, render=True, cmap='plasma')\n",
    "\n",
    "    p.set_viewup([0, 1, 0])\n",
    "    #p.fly_to([5,0,0])\n",
    "\n",
    "    #p.set_position([5.0, -0.01, 7.5])\n",
    "    p.window_size = [1280,480]\n",
    "    #p.save_graphic('./annSU2_'+str(pod.modes.shape[1])+'_modes.pdf')\n",
    "    p.show(interactive_update=False, auto_close=True)\n",
    "    #p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import pyvista as pv\n",
    "import pandas as pd\n",
    "\n",
    "def genQ1DHDF5(path, variables, outputfile, solutions_id):\n",
    "    #path = path_ + 'Q1D/*'\n",
    "    print(f'Reading data from {path}, generating HDF5 file...')\n",
    "    #npaths = len(list(filter( os.path.isdir, glob.glob(f'{path}*') ))) \n",
    "\n",
    "    arrsize = len(np.loadtxt(f\"{path}/{1}/Q1D/outputs/{variables[0]}\"))\n",
    "    arrsize = arrsize*len(variables)\n",
    "\n",
    "    snaps = h5py.File(outputfile,'w')\n",
    "    snaps.create_dataset('snapshots', shape=(arrsize, len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('id', shape=(len(solutions_id)), dtype=np.int32)\n",
    "    #snaps.create_dataset('p0in', shape=(len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('T0in', shape=(len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('thickness', shape=(len(solutions_id)), dtype=np.float64)\n",
    "    \n",
    "    df = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe_params = list(df.keys())\n",
    "\n",
    "    for param in doe_params: \n",
    "        snaps.create_dataset(param, shape=(len(solutions_id)), dtype=type(df[param][0]))\n",
    "   \n",
    "\n",
    "    #for i in range(1, npaths+1):\n",
    "    for i,j in enumerate(solutions_id):\n",
    "        idx0 = 0\n",
    "        idxf = idx0\n",
    "        for var in variables:\n",
    "            #filename = path.split('*')[0]+str(i)+'/outputs/'+var\n",
    "            filename = f\"{path}/{j}/Q1D/outputs/{var}\"\n",
    "            print(f\"reading file {filename}\")\n",
    "            arr = np.loadtxt(filename)\n",
    "            idx0 = idxf\n",
    "            idxf = idx0 + len(arr)\n",
    "            snaps['snapshots'][idx0:idxf,i-1] = arr[:]\n",
    "    \n",
    "        #snaps['DoE'] = df\n",
    "\n",
    "        #snaps['id'][i] = j\n",
    "\n",
    "        doe_params = get_doe_parameters(f\"{path}/doe_lhs.txt\", j)\n",
    "       \n",
    "        for key, val in doe_params.items():\n",
    "            snaps[key][i] = val\n",
    "\n",
    "        #p0in, T0in, thickness = get_boundary_condition(f\"{path}/doe_lhs.txt\", j)\n",
    "        #snaps['p0in'][i] = p0in\n",
    "        #snaps['T0in'][i] = T0in\n",
    "        #snaps['thickness'][i] = thickness\n",
    "\n",
    "    snaps.close()\n",
    "\n",
    "    doe = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe.to_hdf(outputfile, key='DoE', mode='a', append=True)    \n",
    "\n",
    "    print(f'File {outputfile} written')\n",
    "\n",
    "def get_wall_idx(vtkfile):\n",
    "    output = pv.read(vtkfile)\n",
    "    #edges = output.extract_feature_edges()\n",
    "    edges = output\n",
    "    wall_idx = np.where(edges['Heat_Flux'] != 0)[0]\n",
    "    return wall_idx\n",
    "\n",
    "def genSU2HDF5_Fluid(path, vtkfile, variables, outputfile, solutions_id):\n",
    "    #path = path_ + 'SU2/*'\n",
    "    print(f'Reading data from {path}, generating HDF5 file...')\n",
    "    #npaths = len(list(filter( os.path.isdir, glob.glob(f'{path}*') )))\n",
    "\n",
    "    #mesh = pv.read(path.split('*')[0]+'1/outputs/solution.vtk')\n",
    "    mesh = pv.read(f\"{path}/1/SU2/outputs/{vtkfile}\")\n",
    "    if 'Heat_Flux' in variables:\n",
    "        wall_idx = get_wall_idx(f\"{path}/1/SU2/outputs/{vtkfile}\")\n",
    "        arrsize = len(mesh[variables[0]])\n",
    "        arrsize = arrsize*(len(variables)-1) + len(wall_idx)\n",
    "    else:\n",
    "        arrsize = len(mesh[variables[0]])\n",
    "        arrsize = arrsize*len(variables)\n",
    "\n",
    "    snaps = h5py.File(outputfile,'w')\n",
    "    snaps.create_dataset('snapshots', shape=(arrsize, len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('id', shape=(len(solutions_id)), dtype=np.int32)\n",
    "    #snaps.create_dataset('p0in', shape=(len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('T0in', shape=(len(solutions_id)), dtype=np.float64)\n",
    "    #snaps.create_dataset('thickness', shape=(len(solutions_id)), dtype=np.float64)\n",
    "\n",
    "    df = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe_params = list(df.keys())\n",
    "\n",
    "    for param in doe_params: \n",
    "        snaps.create_dataset(param, shape=(len(solutions_id)), dtype=type(df[param][0]))\n",
    "    \n",
    "    meshfiles=[]\n",
    "    for i,j in enumerate(solutions_id):\n",
    "        idx0 = 0\n",
    "        idxf = idx0\n",
    "        #filename = path.split('*')[0]+str(i)+'/outputs/solution.vtk'\n",
    "        filename = f\"{path}/{j}/SU2/outputs/{vtkfile}\"\n",
    "        meshfile = f\"{path}/{j}/SU2/outputs/{vtkfile.split('.vtk')[0]}_mesh.vtk\"\n",
    "        print(f'reading file {filename}')\n",
    "        mesh = pv.read(filename)\n",
    "        for var in variables:\n",
    "            if var == 'Heat_Flux':\n",
    "                arr = mesh[var][wall_idx]\n",
    "                \n",
    "                #import matplotlib.pyplot as plt\n",
    "                #print(var)\n",
    "                #print(mesh[var].shape)\n",
    "                #plotMesh(f\"{path}/{j}/SU2/outputs/{vtkfile}\", mesh[var], var)\n",
    "                #plt.plot(mesh[var][wall_idx])\n",
    "                #plt.show()\n",
    "                #input()\n",
    "                \n",
    "                idx0 = idxf\n",
    "                idxf = idx0 + len(arr)\n",
    "                snaps['snapshots'][idx0:idxf,i-1] = arr[:]\n",
    "            else:\n",
    "                arr = mesh[var]\n",
    "                idx0 = idxf\n",
    "                idxf = idx0 + len(arr)\n",
    "                snaps['snapshots'][idx0:idxf,i-1] = arr[:]\n",
    "        meshfiles.append(meshfile)\n",
    "\n",
    "        #snaps['id'][i] = j\n",
    "\n",
    "        doe_params = get_doe_parameters(f\"{path}/doe_lhs.txt\", j)\n",
    "       \n",
    "        for key, val in doe_params.items():\n",
    "            snaps[key][i] = val\n",
    "\n",
    "        #p0in, T0in, thickness = get_boundary_condition(f\"{path}/doe_lhs.txt\", j)\n",
    "        #snaps['p0in'][i] = p0in\n",
    "        #snaps['T0in'][i] = T0in\n",
    "        #snaps['thickness'][i] = thickness\n",
    "\n",
    "         \n",
    "\n",
    "    snaps.create_dataset(\"meshfile\", data=np.array(meshfiles, dtype='S'))\n",
    "    snaps.close()\n",
    "\n",
    "    doe = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe.to_hdf(outputfile, key='DoE', mode='a', append=True)\n",
    "\n",
    "    print(f'File {outputfile} written')\n",
    "\n",
    "def genSU2HDF5(path, vtkfile, variables, outputfile, solutions_id):\n",
    "    #path = path_ + 'SU2/*'\n",
    "    print(f'Reading data from {path}, generating HDF5 file...')\n",
    "    #npaths = len(list(filter( os.path.isdir, glob.glob(f'{path}*') )))\n",
    "\n",
    "    #mesh = pv.read(path.split('*')[0]+'1/outputs/solution.vtk')\n",
    "    mesh = pv.read(f\"{path}/1/SU2/outputs/{vtkfile}\")\n",
    "    arrsize = len(mesh[variables[0]])\n",
    "    arrsize = arrsize*len(variables)\n",
    "\n",
    "    snaps = h5py.File(outputfile,'w')\n",
    "    snaps.create_dataset('snapshots', shape=(arrsize, len(solutions_id)), dtype=np.float64)\n",
    "    snaps.create_dataset('id', shape=(len(solutions_id)), dtype=np.int32)\n",
    "\n",
    "    df = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe_params = list(df.keys())\n",
    "\n",
    "    for param in doe_params: \n",
    "        snaps.create_dataset(param, shape=(len(solutions_id)), dtype=type(df[param][0]))\n",
    "    \n",
    "        \n",
    "    meshfiles=[]\n",
    "    for i,j in enumerate(solutions_id):\n",
    "        idx0 = 0\n",
    "        idxf = idx0\n",
    "        #filename = path.split('*')[0]+str(i)+'/outputs/solution.vtk'\n",
    "        filename = f\"{path}/{j}/SU2/outputs/{vtkfile}\"\n",
    "        meshfile = f\"{path}/{j}/SU2/outputs/{vtkfile.split('.vtk')[0]}_mesh.vtk\"\n",
    "        print(f'reading file {filename}')\n",
    "        mesh = pv.read(filename)\n",
    "        for var in variables:\n",
    "            arr = mesh[var]\n",
    "            idx0 = idxf\n",
    "            idxf = idx0 + len(arr)\n",
    "            snaps['snapshots'][idx0:idxf,i-1] = arr[:]\n",
    "        meshfiles.append(meshfile)\n",
    "\n",
    "        snaps['id'][i] = j\n",
    "\n",
    "        #p0in, T0in, thickness = get_boundary_condition(f\"{path}/doe_lhs.txt\", j)\n",
    "        doe_params = get_doe_parameters(f\"{path}/doe_lhs.txt\", j)\n",
    "       \n",
    "        for key, val in doe_params.items():\n",
    "            snaps[key][i] = val\n",
    "        \n",
    "        #snaps['p0in'][i] = p0in\n",
    "        #snaps['T0in'][i] = T0in\n",
    "        #snaps['thickness'][i] = thickness\n",
    "\n",
    "    snaps.create_dataset(\"meshfile\", data=np.array(meshfiles, dtype='S'))\n",
    "    snaps.close()\n",
    "\n",
    "    doe = pd.read_csv(f\"{path}/doe_lhs.txt\")\n",
    "    doe.to_hdf(outputfile, key='DoE', mode='a', append=True)\n",
    "\n",
    "    print(f'File {outputfile} written')\n",
    "\n",
    "\n",
    "def _get_converged_solutions(path):\n",
    "    # old implementations, dooest not take the solution filename for verification\n",
    "    solutions_id = []\n",
    "    n_directories = len(list(filter( os.path.isdir, glob.glob(f'{path}/*') ))) \n",
    "    for i in range(n_directories):\n",
    "        solution_file = f'{path}/{i+1}/SU2/outputs/fluid.vtk'\n",
    "        if len(list( glob.glob(f'{solution_file}*') )) > 0:\n",
    "            solutions_id.append(i+1)\n",
    "\n",
    "    return solutions_id\n",
    "\n",
    "def get_converged_solutions(path, after_path):\n",
    "    solutions_id = []\n",
    "    n_directories = len(list(filter( os.path.isdir, glob.glob(f'{path}/*') ))) \n",
    "    for i in range(n_directories):\n",
    "        solution_file = f'{path}/{i+1}{after_path}'\n",
    "        if len(list( glob.glob(f'{solution_file}*') )) > 0:\n",
    "            solutions_id.append(i+1)\n",
    "\n",
    "    return solutions_id\n",
    "\n",
    "def get_boundary_condition(doe_file, id):\n",
    "    #dataset_root = '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200'\n",
    "    #id = 6 # case to run\n",
    "    #doe_file = '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/doe_lhs.txt'\n",
    "    doe_table = np.loadtxt(doe_file, skiprows=1, delimiter=',')\n",
    "    idx = np.where(doe_table[:,0] == id) # idx related to case number\n",
    "\n",
    "    p0in = doe_table[idx,1][0][0]\n",
    "    T0in = doe_table[idx,2][0][0]\n",
    "    thickness = doe_table[idx,3][0][0]\n",
    "\n",
    "    return p0in, T0in, thickness\n",
    "\n",
    "def get_doe_parameters(doe_file, id):\n",
    "    df = pd.read_csv(doe_file, delimiter=',')\n",
    "\n",
    "    d = {}\n",
    "    for key in df.keys():\n",
    "        d[key] = df[df['ID']==id][key][0]\n",
    "    return d\n",
    "    #return tuple(d.values())\n",
    "\n",
    "def main():\n",
    "    \n",
    "    solutions_id = _get_converged_solutions('/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200')\n",
    "\n",
    "    genQ1DHDF5('/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200', \n",
    "               ['p.txt','T.txt','M.txt'],\n",
    "               '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/Q1D.hdf5', \n",
    "               solutions_id)       \n",
    "    \n",
    "    genSU2HDF5_Fluid('/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200', \n",
    "               'fluid.vtk',\n",
    "               ['Pressure','Temperature','Mach','Heat_Flux'],\n",
    "               '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/SU2_fluid.hdf5',\n",
    "               solutions_id)\n",
    "    \n",
    "    #genSU2HDF5('/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200', \n",
    "    #           'solid.vtk',\n",
    "    #           ['Temperature'],\n",
    "    #           '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/SU2_solid.hdf5',\n",
    "    #           solutions_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doe_path = '/home/ppiper/ihtc_repository/data/doe_test'\n",
    "su2_after_path = '/SU2/outputs/cht_setupSU2.vtm'\n",
    "\n",
    "q1d_variables =  ['p.txt','T.txt','M.txt']\n",
    "d1d_hdf5_file =  f'{doe_path}Q1D.hdf5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions_id = get_converged_solutions(path=doe_path, after_path=su2_after_path)\n",
    "solutions_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /home/ppiper/ihtc_repository/data/doe_test, generating HDF5 file...\n",
      "reading file /home/ppiper/ihtc_repository/data/doe_test/1/Q1D/outputs/p.txt\n",
      "reading file /home/ppiper/ihtc_repository/data/doe_test/1/Q1D/outputs/T.txt\n",
      "reading file /home/ppiper/ihtc_repository/data/doe_test/1/Q1D/outputs/M.txt\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pytables'.  Use pip or conda to install pytables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/site-packages/pandas/compat/_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tables'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m genQ1DHDF5( path \u001b[39m=\u001b[39m doe_path, variables\u001b[39m=\u001b[39mq1d_variables, outputfile\u001b[39m=\u001b[39md1d_hdf5_file, \n\u001b[1;32m      2\u001b[0m             solutions_id \u001b[39m=\u001b[39m solutions_id)\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mgenQ1DHDF5\u001b[0;34m(path, variables, outputfile, solutions_id)\u001b[0m\n\u001b[1;32m     56\u001b[0m snaps\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     58\u001b[0m doe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/doe_lhs.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m doe\u001b[39m.\u001b[39mto_hdf(outputfile, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDoE\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)    \n\u001b[1;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00moutputfile\u001b[39m}\u001b[39;00m\u001b[39m written\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/site-packages/pandas/core/generic.py:2682\u001b[0m, in \u001b[0;36mNDFrame.to_hdf\u001b[0;34m(self, path_or_buf, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m pytables\n\u001b[1;32m   2680\u001b[0m \u001b[39m# Argument 3 to \"to_hdf\" has incompatible type \"NDFrame\"; expected\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m# \"Union[DataFrame, Series]\" [arg-type]\u001b[39;00m\n\u001b[0;32m-> 2682\u001b[0m pytables\u001b[39m.\u001b[39mto_hdf(\n\u001b[1;32m   2683\u001b[0m     path_or_buf,\n\u001b[1;32m   2684\u001b[0m     key,\n\u001b[1;32m   2685\u001b[0m     \u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2686\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m   2687\u001b[0m     complevel\u001b[39m=\u001b[39mcomplevel,\n\u001b[1;32m   2688\u001b[0m     complib\u001b[39m=\u001b[39mcomplib,\n\u001b[1;32m   2689\u001b[0m     append\u001b[39m=\u001b[39mappend,\n\u001b[1;32m   2690\u001b[0m     \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m,\n\u001b[1;32m   2691\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[1;32m   2692\u001b[0m     min_itemsize\u001b[39m=\u001b[39mmin_itemsize,\n\u001b[1;32m   2693\u001b[0m     nan_rep\u001b[39m=\u001b[39mnan_rep,\n\u001b[1;32m   2694\u001b[0m     dropna\u001b[39m=\u001b[39mdropna,\n\u001b[1;32m   2695\u001b[0m     data_columns\u001b[39m=\u001b[39mdata_columns,\n\u001b[1;32m   2696\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m   2697\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   2698\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/site-packages/pandas/io/pytables.py:302\u001b[0m, in \u001b[0;36mto_hdf\u001b[0;34m(path_or_buf, key, value, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[1;32m    300\u001b[0m path_or_buf \u001b[39m=\u001b[39m stringify_path(path_or_buf)\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mwith\u001b[39;00m HDFStore(\n\u001b[1;32m    303\u001b[0m         path_or_buf, mode\u001b[39m=\u001b[39mmode, complevel\u001b[39m=\u001b[39mcomplevel, complib\u001b[39m=\u001b[39mcomplib\n\u001b[1;32m    304\u001b[0m     ) \u001b[39mas\u001b[39;00m store:\n\u001b[1;32m    305\u001b[0m         f(store)\n\u001b[1;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/site-packages/pandas/io/pytables.py:560\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mformat is not a defined argument for HDFStore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 560\u001b[0m tables \u001b[39m=\u001b[39m import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mtables\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39mif\u001b[39;00m complib \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m complib \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tables\u001b[39m.\u001b[39mfilters\u001b[39m.\u001b[39mall_complibs:\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    564\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcomplib only supports \u001b[39m\u001b[39m{\u001b[39;00mtables\u001b[39m.\u001b[39mfilters\u001b[39m.\u001b[39mall_complibs\u001b[39m}\u001b[39;00m\u001b[39m compression.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/frenv/lib/python3.11/site-packages/pandas/compat/_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m    146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pytables'.  Use pip or conda to install pytables."
     ]
    }
   ],
   "source": [
    "genQ1DHDF5( path = doe_path, variables=q1d_variables, outputfile=d1d_hdf5_file, \n",
    "            solutions_id = solutions_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     \n",
    "\n",
    "genSU2HDF5_Fluid('/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200', \n",
    "            'fluid.vtk',\n",
    "            ['Pressure','Temperature','Mach','Heat_Flux'],\n",
    "            '/home/ppiper/Dropbox/local/ihtc_nozzle/data/doe_lhs_N200/SU2_fluid.hdf5',\n",
    "            solutions_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "multiblock = pv.read('/home/ppiper/ihtc_repository/data/doe_test/1/SU2/outputs/cht_setupSU2.vtm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock['Zone 0 (Comp. Fluid)']['Internal']['Internal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multiblock['Zone 1 (Solid Heat)']['Boundary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "points = multiblock['Zone 1 (Solid Heat)']['Boundary']['INNERWALL'].points\n",
    "data =  multiblock['Zone 1 (Solid Heat)']['Boundary']['INNERWALL']['Temperature']\n",
    "\n",
    "idx_sort = np.argsort(points[:,0])\n",
    "points[:,0] = points[idx_sort,0]\n",
    "points[:,1] = points[idx_sort,1]\n",
    "data = data[idx_sort]\n",
    "\n",
    "plt.plot(points[:,0], data)\n",
    "#plt.plot(points[:,0], points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pv.Plotter()\n",
    "mesh_fluid = multiblock['Zone 0 (Comp. Fluid)']['Internal']['Internal']\n",
    "mesh_fluid.set_active_scalars('Pressure')\n",
    "mesh_solid = multiblock['Zone 1 (Solid Heat)']['Internal']['Internal']\n",
    "mesh_solid.set_active_scalars('Temperature')\n",
    "pl.add_mesh(mesh_fluid)\n",
    "pl.add_mesh(mesh_solid)\n",
    "pl.show(cpos='xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock['Zone 0 (Comp. Fluid)']['Internal']['Internal']['Pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.GetBlock(0)[1][1]['Density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.GetBlock(0).set_active_scalars('Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.GetBlock(0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock[0][0][0].set_active_scalars('Pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvista import GPUInfo\n",
    "GPUInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(multiblock[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.set_active_scalars('PRESSURE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiblock.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c25544d719d175344236f55a7b30dc3b0a2e294f8af427f95c30844b0f4cb7f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
